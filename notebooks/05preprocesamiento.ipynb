{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e92cd0550dcfd1",
   "metadata": {},
   "source": [
    "# Ejercicio 5: Modelo Probabilístico\n",
    "\n",
    "## Objetivo de la práctica\n",
    "- Aplicar paso a paso técnicas de preprocesamiento, evaluando el impacto de cada etapa en el número de tokens y en el vocabulario final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88982921c8872f8f",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T15:51:10.347651Z",
     "start_time": "2025-05-28T15:51:07.548869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroupsdocs = newsgroups.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6196ea9cb414396",
   "metadata": {},
   "source": [
    "## Parte 1: Tokenización\n",
    "\n",
    "### Actividad \n",
    "1. Tokeniza los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb3a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfb0f2438c9c0144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de documentos tokenizados: 18846\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    return [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "tokenized_docs = tokenize_corpus(newsgroupsdocs)\n",
    "\n",
    "print(\"Número de documentos tokenizados:\", len(tokenized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f24a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de documento Tokenizado:\n",
      "['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'Pens', 'massacre', 'of', 'the', 'Devils', '.', 'Actually', ',', 'I', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.', 'However', ',', 'I', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-PIttsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'Pens', '.', 'Man', ',', 'they', 'are', 'killing', 'those', 'Devils', 'worse', 'than', 'I', 'thought', '.', 'Jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.', 'He', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.', 'Bowman', 'should', 'let', 'JAgr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'Pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'Jersey', 'anyway', '.', 'I', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'Islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.', 'PENS', 'RULE', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo de documento Tokenizado:\")\n",
    "print(tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfc1e6638e9d2",
   "metadata": {},
   "source": [
    "## Parte 2: Normalización\n",
    "\n",
    "### Actividad \n",
    "1. Convierte todos los tokens a minúsculas.\n",
    "2. Elimina puntuación y símbolos no alfabéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc67a424c6550fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de documento en minúsculas:\n",
      "['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils', '.', 'actually', ',', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.', 'however', ',', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-pittsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens', '.', 'man', ',', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought', '.', 'jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.', 'he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.', 'bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', '.', 'i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.', 'pens', 'rule', '!', '!', '!']\n",
      "Ejemplo de documento sin puntuación y símbolos no alfabéticos:\n",
      "['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils', 'actually', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', 'however', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens', 'man', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought', 'jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', 'he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', 'bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', 'i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n"
     ]
    }
   ],
   "source": [
    "#convertir los tokenizados a lower case\n",
    "def lower_case_tokenized_corpus(tokenized_corpus):\n",
    "    return [[word.lower() for word in doc] for doc in tokenized_corpus]\n",
    "lower_case_docs = lower_case_tokenized_corpus(tokenized_docs)\n",
    "print(\"Ejemplo de documento en minúsculas:\")\n",
    "print(lower_case_docs[0])\n",
    "\n",
    "#elimina puntuacion y simbolos no alfabeticos\n",
    "import string\n",
    "def remove_punctuation_and_non_alpha(tokenized_corpus):\n",
    "    return [[word for word in doc if word.isalpha()] for doc in tokenized_corpus]\n",
    "cleaned_docs = remove_punctuation_and_non_alpha(lower_case_docs)\n",
    "print(\"Ejemplo de documento sin puntuación y símbolos no alfabéticos:\")\n",
    "print(cleaned_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973153ad553db841",
   "metadata": {},
   "source": [
    "## Parte 3: Eliminación de Stopwords\n",
    "\n",
    "### Actividad \n",
    "1. Elimina las palabras vacías usando una lista estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "477c7bcd5c2d0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de documento sin stopwords:\n",
      "['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pens', 'man', 'killing', 'devils', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n"
     ]
    }
   ],
   "source": [
    "#con nltk vamos a eliminar stopwords de lo que ya estaba eliminado la puntuación y símbolos no alfabéticos\n",
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(tokenized_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n",
    "cleaned_docs_no_stopwords = remove_stopwords(cleaned_docs)\n",
    "print(\"Ejemplo de documento sin stopwords:\")\n",
    "print(cleaned_docs_no_stopwords[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f090bfc7868f8",
   "metadata": {},
   "source": [
    "## Parte 4: Stemming o Lematización\n",
    "\n",
    "### Actividad\n",
    "1. Aplica stemming.\n",
    "2. Aplica lematización.\n",
    "3. Compara ambas técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd9ff693047bd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de documento con stemming:\n",
      "['sure', 'basher', 'pen', 'fan', 'pretti', 'confus', 'lack', 'kind', 'post', 'recent', 'pen', 'massacr', 'devil', 'actual', 'bit', 'puzzl', 'bit', 'reliev', 'howev', 'go', 'put', 'end', 'relief', 'bit', 'prais', 'pen', 'man', 'kill', 'devil', 'wors', 'thought', 'jagr', 'show', 'much', 'better', 'regular', 'season', 'stat', 'also', 'lot', 'fo', 'fun', 'watch', 'playoff', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'coupl', 'game', 'sinc', 'pen', 'go', 'beat', 'pulp', 'jersey', 'anyway', 'disappoint', 'see', 'island', 'lose', 'final', 'regular', 'season', 'game', 'pen', 'rule']\n"
     ]
    }
   ],
   "source": [
    "#con los stopwords eliminados, vamos a hacer stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem_tokenized_corpus(tokenized_corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [[stemmer.stem(word) for word in doc] for doc in tokenized_corpus]\n",
    "stemmed_docs = stem_tokenized_corpus(cleaned_docs_no_stopwords)\n",
    "print(\"Ejemplo de documento con stemming:\")\n",
    "print(stemmed_docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5857a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ELI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ELI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f63b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de documento con lemmatization:\n",
      "['sure', 'bashers', 'pen', 'fan', 'pretty', 'confused', 'lack', 'kind', 'post', 'recent', 'pen', 'massacre', 'devil', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pen', 'man', 'killing', 'devil', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoff', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'game', 'since', 'pen', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islander', 'lose', 'final', 'regular', 'season', 'game', 'pen', 'rule']\n"
     ]
    }
   ],
   "source": [
    "#con lo de stemming, vamos a hacer lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_tokenized_corpus(tokenized_corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [[lemmatizer.lemmatize(word) for word in doc] for doc in tokenized_corpus]\n",
    "lemmatized_docs = lemmatize_tokenized_corpus(cleaned_docs_no_stopwords)\n",
    "print(\"Ejemplo de documento con lemmatization:\")\n",
    "print(lemmatized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "364135d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación entre stemming y lemmatization:\n",
      "Original: ['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pens', 'man', 'killing', 'devils', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n",
      "Stemming: ['sure', 'basher', 'pen', 'fan', 'pretti', 'confus', 'lack', 'kind', 'post', 'recent', 'pen', 'massacr', 'devil', 'actual', 'bit', 'puzzl', 'bit', 'reliev', 'howev', 'go', 'put', 'end', 'relief', 'bit', 'prais', 'pen', 'man', 'kill', 'devil', 'wors', 'thought', 'jagr', 'show', 'much', 'better', 'regular', 'season', 'stat', 'also', 'lot', 'fo', 'fun', 'watch', 'playoff', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'coupl', 'game', 'sinc', 'pen', 'go', 'beat', 'pulp', 'jersey', 'anyway', 'disappoint', 'see', 'island', 'lose', 'final', 'regular', 'season', 'game', 'pen', 'rule']\n",
      "Lemmatization: ['sure', 'bashers', 'pen', 'fan', 'pretty', 'confused', 'lack', 'kind', 'post', 'recent', 'pen', 'massacre', 'devil', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pen', 'man', 'killing', 'devil', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoff', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'game', 'since', 'pen', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islander', 'lose', 'final', 'regular', 'season', 'game', 'pen', 'rule']\n",
      "\n",
      "Original: ['brother', 'market', 'video', 'card', 'supports', 'vesa', 'local', 'bus', 'ram', 'anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphics', 'ultra', 'pro', 'vlb', 'card', 'please', 'post', 'email', 'thank', 'matt']\n",
      "Stemming: ['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bu', 'ram', 'anyon', 'diamond', 'stealth', 'pro', 'local', 'bu', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra', 'pro', 'vlb', 'card', 'pleas', 'post', 'email', 'thank', 'matt']\n",
      "Lemmatization: ['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bus', 'ram', 'anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra', 'pro', 'vlb', 'card', 'please', 'post', 'email', 'thank', 'matt']\n",
      "\n",
      "Original: ['finally', 'said', 'dream', 'mediterranean', 'new', 'area', 'greater', 'years', 'like', 'holocaust', 'numbers', 'july', 'usa', 'sweden', 'april', 'still', 'cold', 'changed', 'calendar', 'nothing', 'mentioned', 'true', 'let', 'say', 'true', 'shall', 'azeri', 'women', 'children', 'going', 'pay', 'price', 'raped', 'killed', 'tortured', 'armenians', 'hearded', 'something', 'called', 'geneva', 'convention', 'facist', 'ohhh', 'forgot', 'armenians', 'fight', 'nobody', 'forgot', 'killings', 'rapings', 'torture', 'kurds', 'turks', 'upon', 'time', 'ohhhh', 'swedish', 'redcross', 'workers', 'lie', 'ever', 'say', 'regional', 'killer', 'like', 'person', 'shoot', 'policy', 'l', 'confused', 'search', 'turkish', 'planes', 'know', 'talking', 'turkey', 'government', 'announced', 'giving', 'weapons', 'azerbadjan', 'since', 'armenia', 'started', 'attack', 'azerbadjan', 'self', 'karabag', 'province', 'search', 'plane', 'weapons', 'since', 'content', 'announced', 'weapons', 'one', 'confused', 'right', 'give', 'weapons', 'azeris', 'since', 'armenians', 'started', 'fight', 'azerbadjan', 'shoot', 'armenian', 'bread', 'butter', 'arms', 'personel', 'russian', 'army']\n",
      "Stemming: ['final', 'said', 'dream', 'mediterranean', 'new', 'area', 'greater', 'year', 'like', 'holocaust', 'number', 'juli', 'usa', 'sweden', 'april', 'still', 'cold', 'chang', 'calendar', 'noth', 'mention', 'true', 'let', 'say', 'true', 'shall', 'azeri', 'women', 'children', 'go', 'pay', 'price', 'rape', 'kill', 'tortur', 'armenian', 'heard', 'someth', 'call', 'geneva', 'convent', 'facist', 'ohhh', 'forgot', 'armenian', 'fight', 'nobodi', 'forgot', 'kill', 'rape', 'tortur', 'kurd', 'turk', 'upon', 'time', 'ohhhh', 'swedish', 'redcross', 'worker', 'lie', 'ever', 'say', 'region', 'killer', 'like', 'person', 'shoot', 'polici', 'l', 'confus', 'search', 'turkish', 'plane', 'know', 'talk', 'turkey', 'govern', 'announc', 'give', 'weapon', 'azerbadjan', 'sinc', 'armenia', 'start', 'attack', 'azerbadjan', 'self', 'karabag', 'provinc', 'search', 'plane', 'weapon', 'sinc', 'content', 'announc', 'weapon', 'one', 'confus', 'right', 'give', 'weapon', 'azeri', 'sinc', 'armenian', 'start', 'fight', 'azerbadjan', 'shoot', 'armenian', 'bread', 'butter', 'arm', 'personel', 'russian', 'armi']\n",
      "Lemmatization: ['finally', 'said', 'dream', 'mediterranean', 'new', 'area', 'greater', 'year', 'like', 'holocaust', 'number', 'july', 'usa', 'sweden', 'april', 'still', 'cold', 'changed', 'calendar', 'nothing', 'mentioned', 'true', 'let', 'say', 'true', 'shall', 'azeri', 'woman', 'child', 'going', 'pay', 'price', 'raped', 'killed', 'tortured', 'armenian', 'hearded', 'something', 'called', 'geneva', 'convention', 'facist', 'ohhh', 'forgot', 'armenian', 'fight', 'nobody', 'forgot', 'killing', 'rapings', 'torture', 'kurd', 'turk', 'upon', 'time', 'ohhhh', 'swedish', 'redcross', 'worker', 'lie', 'ever', 'say', 'regional', 'killer', 'like', 'person', 'shoot', 'policy', 'l', 'confused', 'search', 'turkish', 'plane', 'know', 'talking', 'turkey', 'government', 'announced', 'giving', 'weapon', 'azerbadjan', 'since', 'armenia', 'started', 'attack', 'azerbadjan', 'self', 'karabag', 'province', 'search', 'plane', 'weapon', 'since', 'content', 'announced', 'weapon', 'one', 'confused', 'right', 'give', 'weapon', 'azeri', 'since', 'armenian', 'started', 'fight', 'azerbadjan', 'shoot', 'armenian', 'bread', 'butter', 'arm', 'personel', 'russian', 'army']\n",
      "\n",
      "Original: ['think', 'scsi', 'card', 'dma', 'transfers', 'disks', 'scsi', 'card', 'dma', 'transfers', 'containing', 'data', 'scsi', 'devices', 'attached', 'wants', 'important', 'feature', 'scsi', 'ability', 'detach', 'device', 'frees', 'scsi', 'bus', 'devices', 'typically', 'used', 'os', 'start', 'transfers', 'several', 'devices', 'device', 'seeking', 'data', 'bus', 'free', 'commands', 'data', 'transfers', 'devices', 'ready', 'transfer', 'data', 'aquire', 'bus', 'send', 'data', 'ide', 'bus', 'start', 'transfer', 'bus', 'busy', 'disk', 'seeked', 'data', 'transfered', 'typically', 'second', 'lock', 'processes', 'wanting', 'bus', 'irrespective', 'transfer', 'time']\n",
      "Stemming: ['think', 'scsi', 'card', 'dma', 'transfer', 'disk', 'scsi', 'card', 'dma', 'transfer', 'contain', 'data', 'scsi', 'devic', 'attach', 'want', 'import', 'featur', 'scsi', 'abil', 'detach', 'devic', 'free', 'scsi', 'bu', 'devic', 'typic', 'use', 'os', 'start', 'transfer', 'sever', 'devic', 'devic', 'seek', 'data', 'bu', 'free', 'command', 'data', 'transfer', 'devic', 'readi', 'transfer', 'data', 'aquir', 'bu', 'send', 'data', 'ide', 'bu', 'start', 'transfer', 'bu', 'busi', 'disk', 'seek', 'data', 'transfer', 'typic', 'second', 'lock', 'process', 'want', 'bu', 'irrespect', 'transfer', 'time']\n",
      "Lemmatization: ['think', 'scsi', 'card', 'dma', 'transfer', 'disk', 'scsi', 'card', 'dma', 'transfer', 'containing', 'data', 'scsi', 'device', 'attached', 'want', 'important', 'feature', 'scsi', 'ability', 'detach', 'device', 'free', 'scsi', 'bus', 'device', 'typically', 'used', 'o', 'start', 'transfer', 'several', 'device', 'device', 'seeking', 'data', 'bus', 'free', 'command', 'data', 'transfer', 'device', 'ready', 'transfer', 'data', 'aquire', 'bus', 'send', 'data', 'ide', 'bus', 'start', 'transfer', 'bus', 'busy', 'disk', 'seeked', 'data', 'transfered', 'typically', 'second', 'lock', 'process', 'wanting', 'bus', 'irrespective', 'transfer', 'time']\n",
      "\n",
      "Original: ['old', 'jasmine', 'drive', 'use', 'new', 'system', 'understanding', 'upsate', 'driver', 'modern', 'one', 'order', 'gain', 'compatability', 'system', 'anyone', 'know', 'inexpensive', 'program', 'seen', 'formatters', 'buit', 'idea', 'work', 'another', 'ancient', 'device', 'one', 'tape', 'drive', 'back', 'utility', 'freezes', 'system', 'try', 'use', 'drive', 'jasmine', 'direct', 'tape', 'bought', 'used', 'tapes', 'techmar', 'mechanism', 'essentially', 'question', 'anyone', 'know', 'inexpensive', 'beckup', 'utility', 'use', 'system']\n",
      "Stemming: ['old', 'jasmin', 'drive', 'use', 'new', 'system', 'understand', 'upsat', 'driver', 'modern', 'one', 'order', 'gain', 'compat', 'system', 'anyon', 'know', 'inexpens', 'program', 'seen', 'formatt', 'buit', 'idea', 'work', 'anoth', 'ancient', 'devic', 'one', 'tape', 'drive', 'back', 'util', 'freez', 'system', 'tri', 'use', 'drive', 'jasmin', 'direct', 'tape', 'bought', 'use', 'tape', 'techmar', 'mechan', 'essenti', 'question', 'anyon', 'know', 'inexpens', 'beckup', 'util', 'use', 'system']\n",
      "Lemmatization: ['old', 'jasmine', 'drive', 'use', 'new', 'system', 'understanding', 'upsate', 'driver', 'modern', 'one', 'order', 'gain', 'compatability', 'system', 'anyone', 'know', 'inexpensive', 'program', 'seen', 'formatters', 'buit', 'idea', 'work', 'another', 'ancient', 'device', 'one', 'tape', 'drive', 'back', 'utility', 'freeze', 'system', 'try', 'use', 'drive', 'jasmine', 'direct', 'tape', 'bought', 'used', 'tape', 'techmar', 'mechanism', 'essentially', 'question', 'anyone', 'know', 'inexpensive', 'beckup', 'utility', 'use', 'system']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compara stemming y lemmatization\n",
    "print(\"Comparación entre stemming y lemmatization:\")\n",
    "for i in range(5):\n",
    "    print(f\"Original: {cleaned_docs_no_stopwords[i]}\")\n",
    "    print(f\"Stemming: {stemmed_docs[i]}\")\n",
    "    print(f\"Lemmatization: {lemmatized_docs[i]}\")\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
