{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f8b5c16e7eb563",
   "metadata": {},
   "source": [
    "# Ejercicio 6: Dense Retrieval e Introducción a FAISS\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Generar embeddings con sentence-transformers (SBERT, E5), e indexar documentos con FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd69ed7fcbeef9d",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus\n",
    "### Actividad\n",
    "\n",
    "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
    "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00fbde6cfc88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Oakland, California, Sunday, April 25th, 1:05 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>\\n\\nNo matter how \"absurd\" it is to suggest th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Anyone here know if NCD is doing educational p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>\\ntoo bad he doesn't bring the ability to hit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>I know that the placebo effect is where a pati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                doc\n",
       "0        0  \\n\\nI am sure some bashers of Pens fans are pr...\n",
       "1        1  My brother is in the market for a high-perform...\n",
       "2        2  \\n\\n\\n\\n\\tFinally you said what you dream abou...\n",
       "3        3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...\n",
       "4        4  1)    I have an old Jasmine drive which I cann...\n",
       "...    ...                                                ...\n",
       "1995  1995  Oakland, California, Sunday, April 25th, 1:05 ...\n",
       "1996  1996  \\n\\nNo matter how \"absurd\" it is to suggest th...\n",
       "1997  1997  Anyone here know if NCD is doing educational p...\n",
       "1998  1998  \\ntoo bad he doesn't bring the ability to hit,...\n",
       "1999  1999  I know that the placebo effect is where a pati...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "#limitar a los primeros 2000 documentos \n",
    "newsgroups.data = newsgroups.data[:2000]\n",
    "newsgroupsdocs = newsgroups.data\n",
    "\n",
    "#mostrar en un dataframe el id y el texto de los documentos\n",
    "corpus_df = pd.DataFrame({'id': range(len(newsgroupsdocs)), 'doc': newsgroupsdocs})\n",
    "corpus_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9184f4b3e66e20a",
   "metadata": {},
   "source": [
    "## Parte 2: Generación de Embeddings\n",
    "### Actividad\n",
    "\n",
    "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
    "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
    "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525ae7515c6169d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E67FAC9BE0>: Failed to establish a new connection: [WinError 10013] Intento de acceso a un socket no permitido por sus permisos de acceso')': /simple/sentence-transformers/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E67FA41BD0>: Failed to establish a new connection: [WinError 10013] Intento de acceso a un socket no permitido por sus permisos de acceso')': /simple/sentence-transformers/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E67FA41D10>: Failed to establish a new connection: [WinError 10013] Intento de acceso a un socket no permitido por sus permisos de acceso')': /simple/sentence-transformers/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E67FA41E50>: Failed to establish a new connection: [WinError 10013] Intento de acceso a un socket no permitido por sus permisos de acceso')': /simple/sentence-transformers/\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750667b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 63/63 [00:44<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "      <th>embeddings_sbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>[0.0020780046470463276, 0.02345043234527111, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>[0.05006030574440956, 0.0269809328019619, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>[0.016404753550887108, 0.08100050687789917, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>[-0.01939147524535656, 0.011494365520775318, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>[-0.03928707540035248, -0.05540286749601364, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Oakland, California, Sunday, April 25th, 1:05 ...</td>\n",
       "      <td>[0.044003989547491074, 0.03598788380622864, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>\\n\\nNo matter how \"absurd\" it is to suggest th...</td>\n",
       "      <td>[-0.08084699511528015, 0.017292389646172523, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Anyone here know if NCD is doing educational p...</td>\n",
       "      <td>[-0.07489252090454102, -0.0004223576979711652,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>\\ntoo bad he doesn't bring the ability to hit,...</td>\n",
       "      <td>[0.0978073701262474, 0.042095087468624115, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>I know that the placebo effect is where a pati...</td>\n",
       "      <td>[0.04761756956577301, -0.017351282760500908, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                doc  \\\n",
       "0        0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1        1  My brother is in the market for a high-perform...   \n",
       "2        2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3        3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4        4  1)    I have an old Jasmine drive which I cann...   \n",
       "...    ...                                                ...   \n",
       "1995  1995  Oakland, California, Sunday, April 25th, 1:05 ...   \n",
       "1996  1996  \\n\\nNo matter how \"absurd\" it is to suggest th...   \n",
       "1997  1997  Anyone here know if NCD is doing educational p...   \n",
       "1998  1998  \\ntoo bad he doesn't bring the ability to hit,...   \n",
       "1999  1999  I know that the placebo effect is where a pati...   \n",
       "\n",
       "                                       embeddings_sbert  \n",
       "0     [0.0020780046470463276, 0.02345043234527111, 0...  \n",
       "1     [0.05006030574440956, 0.0269809328019619, -0.0...  \n",
       "2     [0.016404753550887108, 0.08100050687789917, -0...  \n",
       "3     [-0.01939147524535656, 0.011494365520775318, -...  \n",
       "4     [-0.03928707540035248, -0.05540286749601364, -...  \n",
       "...                                                 ...  \n",
       "1995  [0.044003989547491074, 0.03598788380622864, -0...  \n",
       "1996  [-0.08084699511528015, 0.017292389646172523, -...  \n",
       "1997  [-0.07489252090454102, -0.0004223576979711652,...  \n",
       "1998  [0.0978073701262474, 0.042095087468624115, -0....  \n",
       "1999  [0.04761756956577301, -0.017351282760500908, -...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#obtener los embeddings de los documentos\n",
    "corpus_embeddings = sbert_model.encode(corpus_df['doc'].tolist(), show_progress_bar=True, convert_to_tensor=True)\n",
    "#agregar los embeddings al dataframe\n",
    "corpus_df['embeddings_sbert'] = corpus_embeddings.tolist()\n",
    "#mostrar el tamaño de los embeddings\n",
    "print(corpus_embeddings.shape)\n",
    "#mostrar el dataframe con los embeddings\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a36a8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 63/63 [05:55<00:00,  5.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "      <th>embeddings_sbert</th>\n",
       "      <th>embeddings_e5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>[0.0020780046470463276, 0.02345043234527111, 0...</td>\n",
       "      <td>[-0.057998958975076675, -0.0020638704299926758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>[0.05006030574440956, 0.0269809328019619, -0.0...</td>\n",
       "      <td>[-0.047147322446107864, 0.00045925582526251674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>[0.016404753550887108, 0.08100050687789917, -0...</td>\n",
       "      <td>[-0.03237044811248779, 0.024496663361787796, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>[-0.01939147524535656, 0.011494365520775318, -...</td>\n",
       "      <td>[-0.07731803506612778, 0.017821243032813072, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>[-0.03928707540035248, -0.05540286749601364, -...</td>\n",
       "      <td>[-0.03879633918404579, 0.0034529452677816153, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Oakland, California, Sunday, April 25th, 1:05 ...</td>\n",
       "      <td>[0.044003989547491074, 0.03598788380622864, -0...</td>\n",
       "      <td>[-0.05249633267521858, 0.03624464571475983, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>\\n\\nNo matter how \"absurd\" it is to suggest th...</td>\n",
       "      <td>[-0.08084699511528015, 0.017292389646172523, -...</td>\n",
       "      <td>[-0.006697574630379677, 0.031097760424017906, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Anyone here know if NCD is doing educational p...</td>\n",
       "      <td>[-0.07489252090454102, -0.0004223576979711652,...</td>\n",
       "      <td>[-0.04950818046927452, 0.032965317368507385, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>\\ntoo bad he doesn't bring the ability to hit,...</td>\n",
       "      <td>[0.0978073701262474, 0.042095087468624115, -0....</td>\n",
       "      <td>[-0.07545769214630127, 0.02335001528263092, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>I know that the placebo effect is where a pati...</td>\n",
       "      <td>[0.04761756956577301, -0.017351282760500908, -...</td>\n",
       "      <td>[-0.04039580747485161, 0.0246506929397583, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                doc  \\\n",
       "0        0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1        1  My brother is in the market for a high-perform...   \n",
       "2        2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3        3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4        4  1)    I have an old Jasmine drive which I cann...   \n",
       "...    ...                                                ...   \n",
       "1995  1995  Oakland, California, Sunday, April 25th, 1:05 ...   \n",
       "1996  1996  \\n\\nNo matter how \"absurd\" it is to suggest th...   \n",
       "1997  1997  Anyone here know if NCD is doing educational p...   \n",
       "1998  1998  \\ntoo bad he doesn't bring the ability to hit,...   \n",
       "1999  1999  I know that the placebo effect is where a pati...   \n",
       "\n",
       "                                       embeddings_sbert  \\\n",
       "0     [0.0020780046470463276, 0.02345043234527111, 0...   \n",
       "1     [0.05006030574440956, 0.0269809328019619, -0.0...   \n",
       "2     [0.016404753550887108, 0.08100050687789917, -0...   \n",
       "3     [-0.01939147524535656, 0.011494365520775318, -...   \n",
       "4     [-0.03928707540035248, -0.05540286749601364, -...   \n",
       "...                                                 ...   \n",
       "1995  [0.044003989547491074, 0.03598788380622864, -0...   \n",
       "1996  [-0.08084699511528015, 0.017292389646172523, -...   \n",
       "1997  [-0.07489252090454102, -0.0004223576979711652,...   \n",
       "1998  [0.0978073701262474, 0.042095087468624115, -0....   \n",
       "1999  [0.04761756956577301, -0.017351282760500908, -...   \n",
       "\n",
       "                                          embeddings_e5  \n",
       "0     [-0.057998958975076675, -0.0020638704299926758...  \n",
       "1     [-0.047147322446107864, 0.00045925582526251674...  \n",
       "2     [-0.03237044811248779, 0.024496663361787796, -...  \n",
       "3     [-0.07731803506612778, 0.017821243032813072, -...  \n",
       "4     [-0.03879633918404579, 0.0034529452677816153, ...  \n",
       "...                                                 ...  \n",
       "1995  [-0.05249633267521858, 0.03624464571475983, -0...  \n",
       "1996  [-0.006697574630379677, 0.031097760424017906, ...  \n",
       "1997  [-0.04950818046927452, 0.032965317368507385, 0...  \n",
       "1998  [-0.07545769214630127, 0.02335001528263092, -0...  \n",
       "1999  [-0.04039580747485161, 0.0246506929397583, -0....  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "E5_Model = SentenceTransformer('intfloat/e5-base')\n",
    "#obtener los embeddings de los documentos \n",
    "corpus_embeddings_e5 = E5_Model.encode(\n",
    "    [\"passage: \" + doc for doc in corpus_df['doc'].tolist()],\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "#agregar los embeddings al dataframe\n",
    "corpus_df['embeddings_e5'] = corpus_embeddings_e5.tolist()\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b50365064d2b1",
   "metadata": {},
   "source": [
    "## Parte 3: Indexación con FAISS\n",
    "### Actividad\n",
    "\n",
    "1. Crea un índice plano con faiss.IndexFlatL2 para búsquedas por distancia euclidiana.\n",
    "2. Asegúrate de usar la dimensión correcta `(embedding_dim = doc_embeddings.shape[1])`.\n",
    "3. Agrega los vectores de documentos al índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd8e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from faiss-cpu) (2.2.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.11.0-cp313-cp313-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.0 MB 1.2 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 1.0/15.0 MB 1.7 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.6/15.0 MB 1.8 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.1/15.0 MB 2.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.4/15.0 MB 2.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.9/15.0 MB 2.1 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.1/15.0 MB 2.1 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.7/15.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 4.2/15.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 5.0/15.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.5/15.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.3/15.0 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.1/15.0 MB 2.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.9/15.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.7/15.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.7/15.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.7/15.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.5/15.0 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.1/15.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.8/15.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.6/15.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/15.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c723e6189ab1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Crea un índice plano con faiss.IndexFlatL2 para búsquedas por distancia euclidiana.\n",
    "import faiss\n",
    "# Crear un índice plano para búsquedas por distancia euclidiana\n",
    "index = faiss.IndexFlatL2(corpus_embeddings.shape[1])  # Usa la dimensión de los embeddings\n",
    "# Asegúrate de que los embeddings sean de tipo float32\n",
    "corpus_embeddings = corpus_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "index.add(corpus_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1087c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de vectores en el índice: 2000\n"
     ]
    }
   ],
   "source": [
    "#imprimir el número de vectores en el índice\n",
    "print(f\"Número de vectores en el índice: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40462a067ca2d379",
   "metadata": {},
   "source": [
    "## Parte 4: Consulta Semántica\n",
    "### Actividad\n",
    "\n",
    "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
    "\n",
    "    * \"God, religion, and spirituality\"\n",
    "    * \"space exploration\"\n",
    "    * \"car maintenance\"\n",
    "\n",
    "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
    "3. Recupera los 5 documentos más relevantes con `index.search(...)`.\n",
    "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad085806124c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: God, religion, and spirituality\n",
      "Resultados de la búsqueda:\n",
      "Documento ID: 996, Distancia: 1.1700\n",
      "Texto: \n",
      "\n",
      "\n",
      "\n",
      "Humanist, or sub-humanist? :-)...\n",
      "\n",
      "Documento ID: 282, Distancia: 1.3386\n",
      "Texto: \n",
      "I didn't know God was a secular humanist...\n",
      "\n",
      "Kent...\n",
      "\n",
      "Documento ID: 677, Distancia: 1.3974\n",
      "Texto:  \n",
      "(Deletion)\n",
      " \n",
      "For me, it is a \"I believe no gods exist\" and a \"I don't believe gods exist\".\n",
      " \n",
      "In other words, I think that statements like gods are or somehow interfere\n",
      "with this world are false or meaningless. In Ontology, one can fairly\n",
      "conclude that when \"A exist\" is meaningless A does not exist. Under the\n",
      "Pragmatic definition of truth, \"A exists\" is meaningless makes A exist\n",
      "even logically false.\n",
      " \n",
      "A problem with such statements is that one can't disprove a subjective god\n",
      "by definition, and...\n",
      "\n",
      "Documento ID: 943, Distancia: 1.4245\n",
      "Texto: \n",
      "\n",
      "Atoms are not objective.  They aren't even real.  What scientists call\n",
      "an atom is nothing more than a mathematical model that describes \n",
      "certain physical, observable properties of our surroundings.  All\n",
      "of which is subjective.  \n",
      "\n",
      "What is objective, though, is the approach a scientist \n",
      "takes in discussing his model and his observations.  There\n",
      "is no objective science.  But there is an objective approach\n",
      "which is subjectively selected by the scientist.  Objective\n",
      "in this case means a specified, ...\n",
      "\n",
      "Documento ID: 791, Distancia: 1.4288\n",
      "Texto: Above all, love each other deeply, because love covers over a multitude of\n",
      "sins. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Escribe una consulta en lenguaje natural\n",
    "query = \"God, religion, and spirituality\"\n",
    "\n",
    "# 2. Obtén el embedding de la consulta\n",
    "query_embedding = sbert_model.encode(query, convert_to_tensor=True).cpu().numpy().astype('float32')\n",
    "\n",
    "# 3. Realiza la búsqueda en el índice\n",
    "k = 5  # Número de resultados a recuperar\n",
    "D, I = index.search(query_embedding.reshape(1, -1), k)  # D: distancias, I: índices\n",
    "\n",
    "# 4. Imprime los resultados\n",
    "print(f\"Consulta: {query}\")\n",
    "print(\"Resultados de la búsqueda:\")\n",
    "for i in range(k):\n",
    "    doc_id = I[0][i]  # Índice del documento en el corpus\n",
    "    distance = D[0][i]  # Distancia al vector de consulta\n",
    "    print(f\"Documento ID: {doc_id}, Distancia: {distance:.4f}\")\n",
    "    print(f\"Texto: {corpus_df['doc'][doc_id][:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9e5e7815c7508",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
