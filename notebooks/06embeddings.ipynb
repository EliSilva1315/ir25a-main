{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f8b5c16e7eb563",
   "metadata": {},
   "source": [
    "# Ejercicio 6: Dense Retrieval e Introducción a FAISS\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Generar embeddings con sentence-transformers (SBERT, E5), e indexar documentos con FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd69ed7fcbeef9d",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus\n",
    "### Actividad\n",
    "\n",
    "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
    "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00fbde6cfc88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Oakland, California, Sunday, April 25th, 1:05 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>\\n\\nNo matter how \"absurd\" it is to suggest th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Anyone here know if NCD is doing educational p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>\\ntoo bad he doesn't bring the ability to hit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>I know that the placebo effect is where a pati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                doc\n",
       "0        0  \\n\\nI am sure some bashers of Pens fans are pr...\n",
       "1        1  My brother is in the market for a high-perform...\n",
       "2        2  \\n\\n\\n\\n\\tFinally you said what you dream abou...\n",
       "3        3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...\n",
       "4        4  1)    I have an old Jasmine drive which I cann...\n",
       "...    ...                                                ...\n",
       "1995  1995  Oakland, California, Sunday, April 25th, 1:05 ...\n",
       "1996  1996  \\n\\nNo matter how \"absurd\" it is to suggest th...\n",
       "1997  1997  Anyone here know if NCD is doing educational p...\n",
       "1998  1998  \\ntoo bad he doesn't bring the ability to hit,...\n",
       "1999  1999  I know that the placebo effect is where a pati...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "#limitar a los primeros 2000 documentos \n",
    "newsgroups.data = newsgroups.data[:2000]\n",
    "newsgroupsdocs = newsgroups.data\n",
    "\n",
    "#mostrar en un dataframe el id y el texto de los documentos\n",
    "corpus_df = pd.DataFrame({'id': range(len(newsgroupsdocs)), 'doc': newsgroupsdocs})\n",
    "corpus_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9184f4b3e66e20a",
   "metadata": {},
   "source": [
    "## Parte 2: Generación de Embeddings\n",
    "### Actividad\n",
    "\n",
    "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
    "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
    "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525ae7515c6169d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.7.1-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading torch-2.7.1-cp313-cp313-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.9/216.1 MB 14.0 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 6.6/216.1 MB 15.5 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 11.0/216.1 MB 17.4 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 14.4/216.1 MB 17.1 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 19.1/216.1 MB 18.2 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 24.4/216.1 MB 19.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 28.6/216.1 MB 19.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 32.8/216.1 MB 19.6 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 38.0/216.1 MB 20.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 43.0/216.1 MB 20.4 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 48.0/216.1 MB 20.6 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 53.0/216.1 MB 20.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 57.1/216.1 MB 20.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 61.6/216.1 MB 20.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 66.3/216.1 MB 21.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 71.3/216.1 MB 21.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 76.0/216.1 MB 21.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 81.3/216.1 MB 21.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 85.2/216.1 MB 21.4 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 90.2/216.1 MB 21.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 94.9/216.1 MB 21.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 99.9/216.1 MB 21.6 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 104.9/216.1 MB 21.7 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 109.8/216.1 MB 21.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 114.0/216.1 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 118.5/216.1 MB 21.7 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 122.7/216.1 MB 21.7 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 127.9/216.1 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 132.9/216.1 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 138.1/216.1 MB 22.0 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 143.4/216.1 MB 22.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 147.6/216.1 MB 22.0 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 152.8/216.1 MB 22.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 157.5/216.1 MB 22.1 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 162.3/216.1 MB 22.1 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 166.5/216.1 MB 22.1 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 171.4/216.1 MB 22.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 175.6/216.1 MB 22.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 180.4/216.1 MB 22.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 184.8/216.1 MB 22.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 189.3/216.1 MB 22.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 192.4/216.1 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 195.0/216.1 MB 21.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 197.9/216.1 MB 21.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 201.3/216.1 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 205.0/216.1 MB 21.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 207.4/216.1 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  211.6/216.1 MB 21.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  215.5/216.1 MB 21.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.7/10.5 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 5.2/6.3 MB 24.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.6 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 23.3 MB/s eta 0:00:00\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, setuptools, safetensors, pyyaml, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] El proceso no tiene acceso al archivo porque está siendo utilizado por otro proceso: 'C:\\\\Users\\\\ELI\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python313\\\\site-packages\\\\tokenizers\\\\tokenizers.pyd'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ELI\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "750667b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 63/63 [00:48<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 384])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "      <th>embeddings_sbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>[0.0020780046470463276, 0.02345043234527111, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>[0.05006030574440956, 0.0269809328019619, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>[0.016404753550887108, 0.08100050687789917, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>[-0.01939147524535656, 0.011494365520775318, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>[-0.03928707540035248, -0.05540286749601364, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Oakland, California, Sunday, April 25th, 1:05 ...</td>\n",
       "      <td>[0.044003989547491074, 0.03598788380622864, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>\\n\\nNo matter how \"absurd\" it is to suggest th...</td>\n",
       "      <td>[-0.08084699511528015, 0.017292389646172523, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Anyone here know if NCD is doing educational p...</td>\n",
       "      <td>[-0.07489252090454102, -0.0004223576979711652,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>\\ntoo bad he doesn't bring the ability to hit,...</td>\n",
       "      <td>[0.0978073701262474, 0.042095087468624115, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>I know that the placebo effect is where a pati...</td>\n",
       "      <td>[0.04761756956577301, -0.017351282760500908, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                doc  \\\n",
       "0        0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1        1  My brother is in the market for a high-perform...   \n",
       "2        2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3        3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4        4  1)    I have an old Jasmine drive which I cann...   \n",
       "...    ...                                                ...   \n",
       "1995  1995  Oakland, California, Sunday, April 25th, 1:05 ...   \n",
       "1996  1996  \\n\\nNo matter how \"absurd\" it is to suggest th...   \n",
       "1997  1997  Anyone here know if NCD is doing educational p...   \n",
       "1998  1998  \\ntoo bad he doesn't bring the ability to hit,...   \n",
       "1999  1999  I know that the placebo effect is where a pati...   \n",
       "\n",
       "                                       embeddings_sbert  \n",
       "0     [0.0020780046470463276, 0.02345043234527111, 0...  \n",
       "1     [0.05006030574440956, 0.0269809328019619, -0.0...  \n",
       "2     [0.016404753550887108, 0.08100050687789917, -0...  \n",
       "3     [-0.01939147524535656, 0.011494365520775318, -...  \n",
       "4     [-0.03928707540035248, -0.05540286749601364, -...  \n",
       "...                                                 ...  \n",
       "1995  [0.044003989547491074, 0.03598788380622864, -0...  \n",
       "1996  [-0.08084699511528015, 0.017292389646172523, -...  \n",
       "1997  [-0.07489252090454102, -0.0004223576979711652,...  \n",
       "1998  [0.0978073701262474, 0.042095087468624115, -0....  \n",
       "1999  [0.04761756956577301, -0.017351282760500908, -...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#obtener los embeddings de los documentos\n",
    "corpus_embeddings = sbert_model.encode(corpus_df['doc'].tolist(), show_progress_bar=True, convert_to_tensor=True)\n",
    "#agregar los embeddings al dataframe\n",
    "corpus_df['embeddings_sbert'] = corpus_embeddings.tolist()\n",
    "#mostrar el tamaño de los embeddings\n",
    "print(corpus_embeddings.shape)\n",
    "#mostrar el dataframe con los embeddings\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a36a8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ELI\\.cache\\huggingface\\hub\\models--intfloat--e5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Batches: 100%|██████████| 63/63 [08:05<00:00,  7.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "      <th>embeddings_sbert</th>\n",
       "      <th>embeddings_e5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>[0.0020780046470463276, 0.02345043234527111, 0...</td>\n",
       "      <td>[-0.057998958975076675, -0.0020638704299926758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>[0.05006030574440956, 0.0269809328019619, -0.0...</td>\n",
       "      <td>[-0.047147322446107864, 0.00045925582526251674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>[0.016404753550887108, 0.08100050687789917, -0...</td>\n",
       "      <td>[-0.03237044811248779, 0.024496663361787796, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>[-0.01939147524535656, 0.011494365520775318, -...</td>\n",
       "      <td>[-0.07731803506612778, 0.017821243032813072, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>[-0.03928707540035248, -0.05540286749601364, -...</td>\n",
       "      <td>[-0.03879633918404579, 0.0034529452677816153, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>Oakland, California, Sunday, April 25th, 1:05 ...</td>\n",
       "      <td>[0.044003989547491074, 0.03598788380622864, -0...</td>\n",
       "      <td>[-0.05249633267521858, 0.03624464571475983, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>\\n\\nNo matter how \"absurd\" it is to suggest th...</td>\n",
       "      <td>[-0.08084699511528015, 0.017292389646172523, -...</td>\n",
       "      <td>[-0.006697574630379677, 0.031097760424017906, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Anyone here know if NCD is doing educational p...</td>\n",
       "      <td>[-0.07489252090454102, -0.0004223576979711652,...</td>\n",
       "      <td>[-0.04950818046927452, 0.032965317368507385, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>\\ntoo bad he doesn't bring the ability to hit,...</td>\n",
       "      <td>[0.0978073701262474, 0.042095087468624115, -0....</td>\n",
       "      <td>[-0.07545769214630127, 0.02335001528263092, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>I know that the placebo effect is where a pati...</td>\n",
       "      <td>[0.04761756956577301, -0.017351282760500908, -...</td>\n",
       "      <td>[-0.04039580747485161, 0.0246506929397583, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                doc  \\\n",
       "0        0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1        1  My brother is in the market for a high-perform...   \n",
       "2        2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3        3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4        4  1)    I have an old Jasmine drive which I cann...   \n",
       "...    ...                                                ...   \n",
       "1995  1995  Oakland, California, Sunday, April 25th, 1:05 ...   \n",
       "1996  1996  \\n\\nNo matter how \"absurd\" it is to suggest th...   \n",
       "1997  1997  Anyone here know if NCD is doing educational p...   \n",
       "1998  1998  \\ntoo bad he doesn't bring the ability to hit,...   \n",
       "1999  1999  I know that the placebo effect is where a pati...   \n",
       "\n",
       "                                       embeddings_sbert  \\\n",
       "0     [0.0020780046470463276, 0.02345043234527111, 0...   \n",
       "1     [0.05006030574440956, 0.0269809328019619, -0.0...   \n",
       "2     [0.016404753550887108, 0.08100050687789917, -0...   \n",
       "3     [-0.01939147524535656, 0.011494365520775318, -...   \n",
       "4     [-0.03928707540035248, -0.05540286749601364, -...   \n",
       "...                                                 ...   \n",
       "1995  [0.044003989547491074, 0.03598788380622864, -0...   \n",
       "1996  [-0.08084699511528015, 0.017292389646172523, -...   \n",
       "1997  [-0.07489252090454102, -0.0004223576979711652,...   \n",
       "1998  [0.0978073701262474, 0.042095087468624115, -0....   \n",
       "1999  [0.04761756956577301, -0.017351282760500908, -...   \n",
       "\n",
       "                                          embeddings_e5  \n",
       "0     [-0.057998958975076675, -0.0020638704299926758...  \n",
       "1     [-0.047147322446107864, 0.00045925582526251674...  \n",
       "2     [-0.03237044811248779, 0.024496663361787796, -...  \n",
       "3     [-0.07731803506612778, 0.017821243032813072, -...  \n",
       "4     [-0.03879633918404579, 0.0034529452677816153, ...  \n",
       "...                                                 ...  \n",
       "1995  [-0.05249633267521858, 0.03624464571475983, -0...  \n",
       "1996  [-0.006697574630379677, 0.031097760424017906, ...  \n",
       "1997  [-0.04950818046927452, 0.032965317368507385, 0...  \n",
       "1998  [-0.07545769214630127, 0.02335001528263092, -0...  \n",
       "1999  [-0.04039580747485161, 0.0246506929397583, -0....  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "E5_Model = SentenceTransformer('intfloat/e5-base')\n",
    "#obtener los embeddings de los documentos \n",
    "corpus_embeddings_e5 = E5_Model.encode(\n",
    "    [\"passage: \" + doc for doc in corpus_df['doc'].tolist()],\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "#agregar los embeddings al dataframe\n",
    "corpus_df['embeddings_e5'] = corpus_embeddings_e5.tolist()\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b50365064d2b1",
   "metadata": {},
   "source": [
    "## Parte 3: Indexación con FAISS\n",
    "### Actividad\n",
    "\n",
    "1. Crea un índice plano con faiss.IndexFlatL2 para búsquedas por distancia euclidiana.\n",
    "2. Asegúrate de usar la dimensión correcta `(embedding_dim = doc_embeddings.shape[1])`.\n",
    "3. Agrega los vectores de documentos al índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c723e6189ab1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40462a067ca2d379",
   "metadata": {},
   "source": [
    "## Parte 4: Consulta Semántica\n",
    "### Actividad\n",
    "\n",
    "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
    "\n",
    "    * \"God, religion, and spirituality\"\n",
    "    * \"space exploration\"\n",
    "    * \"car maintenance\"\n",
    "\n",
    "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
    "3. Recupera los 5 documentos más relevantes con `index.search(...)`.\n",
    "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad085806124c709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc9e5e7815c7508",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
