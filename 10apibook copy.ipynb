{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5888edd",
   "metadata": {},
   "source": [
    "Consulta a un libro con retrieval augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b055381",
   "metadata": {},
   "source": [
    "CARGA DE TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caeee2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>577</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>578</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>579</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>580</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page                                                raw\n",
       "0       1  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...\n",
       "1       2         b'Online edition (c)\\n2009 Cambridge UP\\n'\n",
       "2       3  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...\n",
       "3       4  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...\n",
       "4       5  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...\n",
       "..    ...                                                ...\n",
       "576   577  b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...\n",
       "577   578  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...\n",
       "578   579  b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...\n",
       "579   580  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...\n",
       "580   581  b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...\n",
       "\n",
       "[581 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf\n",
    "import pymupdf as fitz\n",
    "\n",
    "# Ruta al archivo PDF\n",
    "path = \"C:\\\\Users\\\\ELI\\\\Downloads\\\\irbookonlinereading.pdf\"\n",
    "doc = pymupdf.open(path)\n",
    "data=[]\n",
    "for i, page in enumerate(doc):\n",
    "    text = page.get_text().encode('utf-8')\n",
    "    if text.strip():\n",
    "        data.append({\n",
    "            \"page\": i + 1,\n",
    "            \"raw\": str(text).strip()\n",
    "        })\n",
    "# Convertir a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ad9d0",
   "metadata": {},
   "source": [
    "TABLA DE CONTENIDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96b5f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡El PDF tiene 256 elementos en la tabla de contenidos!\n",
      "\n",
      "Primeras 10 entradas:\n",
      "Nivel 1: List of Tables (Página 15)\n",
      "Nivel 1: List of Figures (Página 19)\n",
      "Nivel 1: Table of Notation (Página 27)\n",
      "Nivel 1: Preface (Página 31)\n",
      "Nivel 1: Boolean retrieval (Página 38)\n",
      "  Nivel 2: An example information retrieval problem (Página 40)\n",
      "  Nivel 2: A first take at building an inverted index (Página 43)\n",
      "  Nivel 2: Processing Boolean queries (Página 47)\n",
      "  Nivel 2: The extended Boolean model versus ranked retrieval (Página 51)\n",
      "  Nivel 2: References and further reading (Página 54)\n",
      "Nivel 1: The term vocabulary and postings lists (Página 56)\n",
      "  Nivel 2: Document delineation and character sequence decoding (Página 56)\n",
      "    Nivel 3: Obtaining the character sequence in a document (Página 56)\n",
      "    Nivel 3: Choosing a document unit (Página 57)\n",
      "  Nivel 2: Determining the vocabulary of terms (Página 59)\n",
      "    Nivel 3: Tokenization (Página 59)\n",
      "    Nivel 3: Dropping common terms: stop words (Página 64)\n",
      "    Nivel 3: Normalization (equivalence classing of terms) (Página 65)\n",
      "    Nivel 3: Stemming and lemmatization (Página 69)\n",
      "  Nivel 2: Faster postings list intersection via skip pointers (Página 73)\n",
      "  Nivel 2: Positional postings and phrase queries (Página 76)\n",
      "    Nivel 3: Biword indexes (Página 76)\n",
      "    Nivel 3: Positional indexes (Página 78)\n",
      "    Nivel 3: Combination schemes (Página 80)\n",
      "  Nivel 2: References and further reading (Página 82)\n",
      "Nivel 1: Dictionaries and tolerant retrieval (Página 86)\n",
      "  Nivel 2: Search structures for dictionaries (Página 86)\n",
      "  Nivel 2: Wildcard queries (Página 88)\n",
      "    Nivel 3: General wildcard queries (Página 90)\n",
      "    Nivel 3: k-gram indexes for wildcard queries (Página 91)\n",
      "  Nivel 2: Spelling correction (Página 93)\n",
      "    Nivel 3: Implementing spelling correction (Página 94)\n",
      "    Nivel 3: Forms of spelling correction (Página 94)\n",
      "    Nivel 3: Edit distance (Página 95)\n",
      "    Nivel 3: k-gram indexes for spelling correction (Página 97)\n",
      "    Nivel 3: Context sensitive spelling correction (Página 99)\n",
      "  Nivel 2: Phonetic correction (Página 100)\n",
      "  Nivel 2: References and further reading (Página 102)\n",
      "Nivel 1: Index construction (Página 104)\n",
      "  Nivel 2: Hardware basics (Página 105)\n",
      "  Nivel 2: Blocked sort-based indexing (Página 106)\n",
      "  Nivel 2: Single-pass in-memory indexing (Página 110)\n",
      "  Nivel 2: Distributed indexing (Página 111)\n",
      "  Nivel 2: Dynamic indexing (Página 115)\n",
      "  Nivel 2: Other types of indexes (Página 117)\n",
      "  Nivel 2: References and further reading (Página 120)\n",
      "Nivel 1: Index compression (Página 122)\n",
      "  Nivel 2: Statistical properties of terms in information retrieval (Página 123)\n",
      "    Nivel 3: Heaps' law: Estimating the number of terms (Página 125)\n",
      "    Nivel 3: Zipf's law: Modeling the distribution of terms (Página 126)\n",
      "  Nivel 2: Dictionary compression (Página 127)\n",
      "    Nivel 3: Dictionary as a string (Página 128)\n",
      "    Nivel 3: Blocked storage (Página 129)\n",
      "  Nivel 2: Postings file compression (Página 132)\n",
      "    Nivel 3: Variable byte codes (Página 133)\n",
      "    Nivel 3: Gamma codes (Página 135)\n",
      "  Nivel 2: References and further reading (Página 142)\n",
      "Nivel 1: Scoring, term weighting and the vector space model (Página 146)\n",
      "  Nivel 2: Parametric and zone indexes (Página 147)\n",
      "    Nivel 3: Weighted zone scoring (Página 149)\n",
      "    Nivel 3: Learning weights (Página 150)\n",
      "    Nivel 3: The optimal weight g (Página 152)\n",
      "  Nivel 2: Term frequency and weighting (Página 154)\n",
      "    Nivel 3: Inverse document frequency (Página 154)\n",
      "    Nivel 3: Tf-idf weighting (Página 155)\n",
      "  Nivel 2: The vector space model for scoring (Página 157)\n",
      "    Nivel 3: Dot products (Página 157)\n",
      "    Nivel 3: Queries as vectors (Página 160)\n",
      "    Nivel 3: Computing vector scores (Página 161)\n",
      "  Nivel 2: Variant tf-idf functions (Página 163)\n",
      "    Nivel 3: Sublinear tf scaling (Página 163)\n",
      "    Nivel 3: Maximum tf normalization (Página 164)\n",
      "    Nivel 3: Document and query weighting schemes (Página 165)\n",
      "    Nivel 3: Pivoted normalized document length (Página 166)\n",
      "  Nivel 2: References and further reading (Página 170)\n",
      "Nivel 1: Computing scores in a complete search system (Página 172)\n",
      "  Nivel 2: Efficient scoring and ranking (Página 172)\n",
      "    Nivel 3: Inexact top K document retrieval (Página 174)\n",
      "    Nivel 3: Index elimination (Página 174)\n",
      "    Nivel 3: Champion lists (Página 175)\n",
      "    Nivel 3: Static quality scores and ordering (Página 175)\n",
      "    Nivel 3: Impact ordering (Página 177)\n",
      "    Nivel 3: Cluster pruning (Página 178)\n",
      "  Nivel 2: Components of an information retrieval system (Página 180)\n",
      "    Nivel 3: Tiered indexes (Página 180)\n",
      "    Nivel 3: Query-term proximity (Página 181)\n",
      "    Nivel 3: Designing parsing and scoring functions (Página 182)\n",
      "    Nivel 3: Putting it all together (Página 183)\n",
      "  Nivel 2: Vector space scoring and query operator interaction (Página 184)\n",
      "  Nivel 2: References and further reading (Página 186)\n",
      "Nivel 1: Evaluation in information retrieval (Página 188)\n",
      "  Nivel 2: Information retrieval system evaluation (Página 189)\n",
      "  Nivel 2: Standard test collections (Página 190)\n",
      "  Nivel 2: Evaluation of unranked retrieval sets (Página 191)\n",
      "  Nivel 2: Evaluation of ranked retrieval results (Página 195)\n",
      "  Nivel 2: Assessing relevance (Página 201)\n",
      "    Nivel 3: Critiques and justifications of the concept of relevance (Página 203)\n",
      "  Nivel 2: A broader perspective: System quality and user utility (Página 205)\n",
      "    Nivel 3: System issues (Página 205)\n",
      "    Nivel 3: User utility (Página 206)\n"
     ]
    }
   ],
   "source": [
    "# Verificar si el PDF tiene tabla de contenidos (TOC)\n",
    "toc = doc.get_toc()\n",
    "\n",
    "if toc:\n",
    "    print(f\"¡El PDF tiene {len(toc)} elementos en la tabla de contenidos!\")\n",
    "    print(\"\\nPrimeras 10 entradas:\")\n",
    "    for i, (level, title, page) in enumerate(toc[:100]):\n",
    "        indent = \"  \" * (level - 1)\n",
    "        print(f\"{indent}Nivel {level}: {title} (Página {page})\")\n",
    "else:\n",
    "    print(\"El PDF no tiene tabla de contenidos estructurada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0d2a0",
   "metadata": {},
   "source": [
    "PREPROCESAR EL TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfb064d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>raw</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n'</td>\n",
       "      <td>bonline edition cn2009 cambridge upn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraftndo n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraft xc2x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>577</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn540nindexn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>578</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn541n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>579</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn542nindexn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>580</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn543n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn544nindexn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page                                                raw  \\\n",
       "0       1  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...   \n",
       "1       2         b'Online edition (c)\\n2009 Cambridge UP\\n'   \n",
       "2       3  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...   \n",
       "3       4  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...   \n",
       "4       5  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...   \n",
       "..    ...                                                ...   \n",
       "576   577  b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...   \n",
       "577   578  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...   \n",
       "578   579  b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...   \n",
       "579   580  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...   \n",
       "580   581  b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...   \n",
       "\n",
       "                                            normalized  \n",
       "0    bonline edition cn2009 cambridge upnannintrodu...  \n",
       "1                 bonline edition cn2009 cambridge upn  \n",
       "2    bonline edition cn2009 cambridge upnannintrodu...  \n",
       "3    bonline edition cn2009 cambridge upndraftndo n...  \n",
       "4    bonline edition cn2009 cambridge upndraft xc2x...  \n",
       "..                                                 ...  \n",
       "576  bonline edition cn2009 cambridge upn540nindexn...  \n",
       "577  bonline edition cn2009 cambridge upnindexn541n...  \n",
       "578  bonline edition cn2009 cambridge upn542nindexn...  \n",
       "579  bonline edition cn2009 cambridge upnindexn543n...  \n",
       "580  bonline edition cn2009 cambridge upn544nindexn...  \n",
       "\n",
       "[581 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocesar el texto\n",
    "import re\n",
    "#normalizar texto\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Eliminar espacios en blanco extra\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar caracteres especiales\n",
    "    return text\n",
    "# Aplicar normalización al DataFrame\n",
    "df['normalized'] = df['raw'].apply(normalize_text)\n",
    "# Verificar el resultado\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0c2d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>raw</th>\n",
       "      <th>normalized</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n'</td>\n",
       "      <td>bonline edition cn2009 cambridge upn</td>\n",
       "      <td>bonline edition cn2009 cambridge upn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraftndo n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraftndo d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraft xc2x...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraft xc2x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>577</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn540nindexn...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn540nindexn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>578</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn541n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn541n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>579</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn542nindexn...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn542nindexn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>580</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn543n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn543n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn544nindexn...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn544nindexn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page                                                raw  \\\n",
       "0       1  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...   \n",
       "1       2         b'Online edition (c)\\n2009 Cambridge UP\\n'   \n",
       "2       3  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...   \n",
       "3       4  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...   \n",
       "4       5  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...   \n",
       "..    ...                                                ...   \n",
       "576   577  b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...   \n",
       "577   578  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...   \n",
       "578   579  b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...   \n",
       "579   580  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...   \n",
       "580   581  b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...   \n",
       "\n",
       "                                            normalized  \\\n",
       "0    bonline edition cn2009 cambridge upnannintrodu...   \n",
       "1                 bonline edition cn2009 cambridge upn   \n",
       "2    bonline edition cn2009 cambridge upnannintrodu...   \n",
       "3    bonline edition cn2009 cambridge upndraftndo n...   \n",
       "4    bonline edition cn2009 cambridge upndraft xc2x...   \n",
       "..                                                 ...   \n",
       "576  bonline edition cn2009 cambridge upn540nindexn...   \n",
       "577  bonline edition cn2009 cambridge upnindexn541n...   \n",
       "578  bonline edition cn2009 cambridge upn542nindexn...   \n",
       "579  bonline edition cn2009 cambridge upnindexn543n...   \n",
       "580  bonline edition cn2009 cambridge upn544nindexn...   \n",
       "\n",
       "                                              filtered  \n",
       "0    bonline edition cn2009 cambridge upnannintrodu...  \n",
       "1                 bonline edition cn2009 cambridge upn  \n",
       "2    bonline edition cn2009 cambridge upnannintrodu...  \n",
       "3    bonline edition cn2009 cambridge upndraftndo d...  \n",
       "4    bonline edition cn2009 cambridge upndraft xc2x...  \n",
       "..                                                 ...  \n",
       "576  bonline edition cn2009 cambridge upn540nindexn...  \n",
       "577  bonline edition cn2009 cambridge upnindexn541n...  \n",
       "578  bonline edition cn2009 cambridge upn542nindexn...  \n",
       "579  bonline edition cn2009 cambridge upnindexn543n...  \n",
       "580  bonline edition cn2009 cambridge upn544nindexn...  \n",
       "\n",
       "[581 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminar stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Descargar stopwords si no están disponibles\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Definir stopwords en español\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Función para eliminar stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "# Aplicar eliminación de stopwords al DataFrame\n",
    "df['filtered'] = df['normalized'].apply(remove_stopwords)\n",
    "# Verificar el resultado\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f942a6",
   "metadata": {},
   "source": [
    "EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cdf29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5946cba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001BDF05D9350>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 848eec39-e852-4d2a-a5d4-27380a2d3685)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:962\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    961\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    963\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x000001BDF05D9350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001BDF05D9350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#generar funcion embeddings para todo el texto\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_embeddings\u001b[39m(text):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:85\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1049\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1045\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1046\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1047\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1048\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1957\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1953\u001b[39m                         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   1954\u001b[39m                             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1955\u001b[39m                         )\n\u001b[32m   1956\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1957\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1963\u001b[39m                     vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1965\u001b[39m \u001b[38;5;66;03m# Get files from url, cache, or disk depending on the case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:161\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:161\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3168\u001b[39m, in \u001b[36mHfApi.list_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3166\u001b[39m encoded_path_in_repo = \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + quote(path_in_repo, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3167\u001b[39m tree_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3168\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaginate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpand\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mRepoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRepoFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:36\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[33;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m r = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m hf_raise_for_status(r)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ELI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    697\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001BDF05D9350>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 848eec39-e852-4d2a-a5d4-27380a2d3685)')"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#generar funcion embeddings para todo el texto\n",
    "def generate_embeddings(text):\n",
    "    return model.encode(text)\n",
    "# Aplicar la función de embeddings al DataFrame\n",
    "df['embeddings'] = df['filtered'].apply(generate_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff5eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>raw</th>\n",
       "      <th>normalized</th>\n",
       "      <th>filtered</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "      <td>[-0.11581327, -0.04094364, 0.023199067, -0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n'</td>\n",
       "      <td>bonline edition cn2009 cambridge upn</td>\n",
       "      <td>bonline edition cn2009 cambridge upn</td>\n",
       "      <td>[-0.1323803, -0.023347827, 0.06720841, -0.0928...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnannintrodu...</td>\n",
       "      <td>[0.00705546, -0.048072208, 0.007882582, -0.068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraftndo n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraftndo d...</td>\n",
       "      <td>[-0.05006524, -0.0019235705, -0.016345194, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraft xc2x...</td>\n",
       "      <td>bonline edition cn2009 cambridge upndraft xc2x...</td>\n",
       "      <td>[0.027868386, -0.087042965, -0.01009805, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>577</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn540nindexn...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn540nindexn...</td>\n",
       "      <td>[-0.026038677, -0.12275319, -0.014319798, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>578</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn541n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn541n...</td>\n",
       "      <td>[0.02994777, -0.13199653, -0.008621623, -0.010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>579</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn542nindexn...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn542nindexn...</td>\n",
       "      <td>[0.0034598007, -0.077709176, -0.0045174663, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>580</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\nIndex...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn543n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upnindexn543n...</td>\n",
       "      <td>[-0.020311879, -0.13165753, 0.01688339, -0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn544nindexn...</td>\n",
       "      <td>bonline edition cn2009 cambridge upn544nindexn...</td>\n",
       "      <td>[0.009277609, -0.12823053, 0.015174442, 0.0130...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page                                                raw  \\\n",
       "0       1  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...   \n",
       "1       2         b'Online edition (c)\\n2009 Cambridge UP\\n'   \n",
       "2       3  b'Online edition (c)\\n2009 Cambridge UP\\nAn\\nI...   \n",
       "3       4  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...   \n",
       "4       5  b'Online edition (c)\\n2009 Cambridge UP\\nDRAFT...   \n",
       "..    ...                                                ...   \n",
       "576   577  b'Online edition (c)\\n2009 Cambridge UP\\n540\\n...   \n",
       "577   578  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...   \n",
       "578   579  b'Online edition (c)\\n2009 Cambridge UP\\n542\\n...   \n",
       "579   580  b'Online edition (c)\\n2009 Cambridge UP\\nIndex...   \n",
       "580   581  b'Online edition (c)\\n2009 Cambridge UP\\n544\\n...   \n",
       "\n",
       "                                            normalized  \\\n",
       "0    bonline edition cn2009 cambridge upnannintrodu...   \n",
       "1                 bonline edition cn2009 cambridge upn   \n",
       "2    bonline edition cn2009 cambridge upnannintrodu...   \n",
       "3    bonline edition cn2009 cambridge upndraftndo n...   \n",
       "4    bonline edition cn2009 cambridge upndraft xc2x...   \n",
       "..                                                 ...   \n",
       "576  bonline edition cn2009 cambridge upn540nindexn...   \n",
       "577  bonline edition cn2009 cambridge upnindexn541n...   \n",
       "578  bonline edition cn2009 cambridge upn542nindexn...   \n",
       "579  bonline edition cn2009 cambridge upnindexn543n...   \n",
       "580  bonline edition cn2009 cambridge upn544nindexn...   \n",
       "\n",
       "                                              filtered  \\\n",
       "0    bonline edition cn2009 cambridge upnannintrodu...   \n",
       "1                 bonline edition cn2009 cambridge upn   \n",
       "2    bonline edition cn2009 cambridge upnannintrodu...   \n",
       "3    bonline edition cn2009 cambridge upndraftndo d...   \n",
       "4    bonline edition cn2009 cambridge upndraft xc2x...   \n",
       "..                                                 ...   \n",
       "576  bonline edition cn2009 cambridge upn540nindexn...   \n",
       "577  bonline edition cn2009 cambridge upnindexn541n...   \n",
       "578  bonline edition cn2009 cambridge upn542nindexn...   \n",
       "579  bonline edition cn2009 cambridge upnindexn543n...   \n",
       "580  bonline edition cn2009 cambridge upn544nindexn...   \n",
       "\n",
       "                                            embeddings  \n",
       "0    [-0.11581327, -0.04094364, 0.023199067, -0.036...  \n",
       "1    [-0.1323803, -0.023347827, 0.06720841, -0.0928...  \n",
       "2    [0.00705546, -0.048072208, 0.007882582, -0.068...  \n",
       "3    [-0.05006524, -0.0019235705, -0.016345194, -0....  \n",
       "4    [0.027868386, -0.087042965, -0.01009805, 0.016...  \n",
       "..                                                 ...  \n",
       "576  [-0.026038677, -0.12275319, -0.014319798, -0.0...  \n",
       "577  [0.02994777, -0.13199653, -0.008621623, -0.010...  \n",
       "578  [0.0034598007, -0.077709176, -0.0045174663, -0...  \n",
       "579  [-0.020311879, -0.13165753, 0.01688339, -0.020...  \n",
       "580  [0.009277609, -0.12823053, 0.015174442, 0.0130...  \n",
       "\n",
       "[581 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39de64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11581327, -0.04094364,  0.02319907, ..., -0.0260279 ,\n",
       "         0.02207126, -0.04298127],\n",
       "       [-0.1323803 , -0.02334783,  0.06720841, ...,  0.01590319,\n",
       "        -0.02216803,  0.01160852],\n",
       "       [ 0.00705546, -0.04807221,  0.00788258, ...,  0.0255634 ,\n",
       "        -0.02932286,  0.00203923],\n",
       "       ...,\n",
       "       [ 0.0034598 , -0.07770918, -0.00451747, ..., -0.01676263,\n",
       "        -0.03796598, -0.0576323 ],\n",
       "       [-0.02031188, -0.13165753,  0.01688339, ..., -0.06474322,\n",
       "        -0.04680839, -0.02762038],\n",
       "       [ 0.00927761, -0.12823053,  0.01517444, ..., -0.02842616,\n",
       "        -0.03841518, -0.03689068]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sacar los embeddings del df\n",
    "embeddings = np.array(df['embeddings'].tolist())\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a67575",
   "metadata": {},
   "source": [
    "QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f55f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.49560859e-03,  1.38110016e-02,  3.26872654e-02, -3.50735500e-03,\n",
       "       -4.26756963e-03, -6.79441215e-03,  1.45399123e-01, -1.11046331e-02,\n",
       "       -7.86744952e-02,  2.04429794e-02,  6.93996670e-03, -2.10056063e-02,\n",
       "        4.66357656e-02, -4.85497192e-02, -7.59261772e-02,  8.11752751e-02,\n",
       "        4.30341251e-02,  4.64086644e-02, -9.42633674e-02, -9.08575580e-03,\n",
       "        6.45952672e-02,  1.34171650e-01,  2.02286150e-02,  2.76836660e-02,\n",
       "       -1.09036537e-02,  5.80462441e-02, -9.04709026e-02, -6.54042652e-03,\n",
       "        6.67781010e-02, -8.32427386e-03,  2.57875044e-02,  2.84907110e-02,\n",
       "        6.08631112e-02,  2.46394724e-02,  2.32363176e-02, -3.44091505e-02,\n",
       "       -1.05311200e-01,  5.39251119e-02,  2.46874914e-02, -1.99775379e-02,\n",
       "       -7.40725473e-02, -1.03850476e-01, -1.05458960e-01, -4.92498577e-02,\n",
       "        1.12935327e-01, -8.43069144e-03, -2.37405039e-02,  1.15957484e-02,\n",
       "        5.11254892e-02,  2.55397037e-02, -7.89895132e-02, -5.44321947e-02,\n",
       "       -2.77857948e-02,  4.07745168e-02,  5.46652041e-02, -4.98606823e-02,\n",
       "        5.50606027e-02, -6.10227510e-03,  1.57128684e-02, -3.19950171e-02,\n",
       "        5.04348166e-02, -6.95450418e-03, -1.12563029e-01,  1.59812476e-02,\n",
       "       -2.66369041e-02,  1.33425118e-02, -3.35357971e-02,  4.11395580e-02,\n",
       "        9.86386463e-03,  1.04485288e-01,  3.58191156e-03,  5.76096214e-02,\n",
       "        4.18155789e-02,  8.82757530e-02,  5.10127209e-02,  3.12808007e-02,\n",
       "        4.67001274e-02, -2.38265209e-02,  1.05396658e-01, -8.29283055e-03,\n",
       "       -3.11776735e-02, -5.82809336e-02,  3.25968042e-02, -3.96665670e-02,\n",
       "       -2.62721330e-02,  1.76127777e-02,  4.27872464e-02, -4.91569750e-02,\n",
       "        4.58962470e-02, -5.32164760e-02, -7.92004988e-02, -1.31060258e-01,\n",
       "        5.37770204e-02, -2.63286252e-02, -5.56075349e-02, -5.27327210e-02,\n",
       "       -1.01271033e-01, -5.77392019e-02,  3.00756749e-02,  1.52967855e-01,\n",
       "        4.36527766e-02,  1.37710124e-02, -2.15698145e-02, -4.30005528e-02,\n",
       "       -7.64405727e-02, -6.25144765e-02, -1.38545986e-02,  7.09717199e-02,\n",
       "       -5.38371783e-03, -1.46295847e-02, -7.02142417e-02,  1.53684169e-02,\n",
       "       -6.35770261e-02, -5.10385185e-02, -9.61093698e-03,  4.50118221e-02,\n",
       "        1.13295391e-02,  9.03673768e-02,  4.54282314e-02,  1.80453584e-02,\n",
       "        4.68757004e-03,  9.33001563e-02,  1.58547685e-02,  1.42760975e-02,\n",
       "       -5.64333349e-02, -7.01242536e-02,  3.12203579e-02, -3.39773887e-33,\n",
       "       -1.68736149e-02, -5.03039993e-02, -4.84312028e-02,  4.21389192e-02,\n",
       "        9.89233144e-03,  1.65894870e-02, -4.47337255e-02,  3.62527668e-02,\n",
       "        4.89242524e-02,  5.31930886e-02,  2.69234516e-02, -5.86375706e-02,\n",
       "        3.38096637e-03, -5.39196916e-02,  1.48222193e-01, -4.54560891e-02,\n",
       "        4.32491526e-02, -3.82457074e-04,  9.02365719e-04, -2.41772588e-02,\n",
       "        4.30193841e-02, -5.24586551e-02,  2.12449022e-02,  7.85362441e-04,\n",
       "       -5.80606163e-02, -3.14322896e-02, -6.52769804e-02, -6.14617541e-02,\n",
       "        3.39590311e-02,  6.48514926e-02, -1.97879039e-02, -3.96132544e-02,\n",
       "        5.66153154e-02,  5.45336269e-02,  1.37083381e-02, -3.03796716e-02,\n",
       "        5.51254628e-03, -5.84277548e-02, -1.02149397e-02, -1.17780507e-01,\n",
       "       -1.23385645e-01,  2.44554728e-02, -6.51074499e-02, -7.44798034e-03,\n",
       "        1.07660562e-01, -2.30963919e-02,  1.82456374e-02, -7.18064886e-03,\n",
       "        7.40239862e-03,  4.80398387e-02,  4.54193205e-02,  4.44028042e-02,\n",
       "       -4.42108735e-02,  5.61995320e-02, -1.80403907e-02,  2.04424057e-02,\n",
       "        1.68797262e-02,  2.19390634e-02, -5.13186231e-02, -4.54904474e-02,\n",
       "       -4.12525237e-02,  1.05924271e-02,  3.89571898e-02, -9.01019853e-03,\n",
       "       -4.05898839e-02, -3.11625618e-02, -2.60141622e-02,  3.40113863e-02,\n",
       "       -4.06838469e-02,  4.79287095e-02, -2.62019001e-02, -3.75996120e-02,\n",
       "        3.72779444e-02,  7.93791004e-03, -3.24457593e-04, -3.44919823e-02,\n",
       "        2.18065232e-02,  2.49705520e-02,  7.23750936e-03,  1.06472634e-02,\n",
       "        5.28212972e-02, -1.58585031e-02, -3.98166552e-02, -1.80337206e-02,\n",
       "       -2.72819940e-02, -4.59191613e-02,  2.86110509e-02, -1.03648216e-01,\n",
       "        4.14668992e-02,  6.31220043e-02, -1.31775707e-01,  8.09592828e-02,\n",
       "       -1.78688634e-02, -3.35996039e-02, -1.24460876e-01,  2.18873246e-33,\n",
       "       -6.25533238e-02, -3.27281170e-02, -3.70334946e-02,  2.62979176e-02,\n",
       "       -4.97802123e-02,  8.88015493e-04,  6.72134310e-02,  6.22418411e-02,\n",
       "        3.74592990e-02,  2.50373036e-02,  4.93161604e-02, -2.25159451e-02,\n",
       "        8.28313117e-04,  8.94823857e-03,  4.87676039e-02, -3.46517861e-02,\n",
       "        1.19102508e-01,  6.16159774e-02,  2.85450625e-03,  1.04769610e-01,\n",
       "       -6.72924221e-02, -2.75330823e-02, -8.85441452e-02, -2.21933667e-02,\n",
       "       -3.71344760e-02,  2.62090880e-02, -3.46328542e-02,  2.59234142e-02,\n",
       "       -2.57963547e-03,  2.61633433e-02,  9.08600166e-03,  5.81263043e-02,\n",
       "       -3.99809778e-02,  1.45678446e-02, -3.57088223e-02,  8.69376212e-03,\n",
       "       -3.24679017e-02,  2.67016552e-02, -6.45604506e-02,  7.78672099e-02,\n",
       "        5.25709875e-02,  2.37235148e-03, -5.19191958e-02,  3.72743644e-02,\n",
       "       -3.81415822e-02, -4.92513627e-02, -7.25615695e-02, -1.47923790e-02,\n",
       "        3.99299152e-02,  4.12567556e-02, -4.21517044e-02,  5.74369468e-02,\n",
       "       -4.30727052e-03, -1.79542322e-02,  1.10901771e-02, -7.65947774e-02,\n",
       "       -7.11048394e-02, -7.13938028e-02, -7.50903040e-02, -4.71375510e-02,\n",
       "        2.29640342e-02, -7.45739834e-03, -5.36899222e-03,  7.61548504e-02,\n",
       "        7.49299452e-02, -3.43900137e-02, -8.11513066e-02,  4.90519628e-02,\n",
       "        1.22714611e-02, -1.88819487e-02,  5.20869419e-02,  1.18928440e-02,\n",
       "       -8.58671889e-02, -2.94089108e-03, -9.28990617e-02, -6.82959799e-03,\n",
       "       -1.07445158e-01, -5.26459143e-02,  3.31977047e-02, -6.22288287e-02,\n",
       "        5.43723851e-02,  2.27718595e-02,  2.50977948e-02,  5.46200313e-02,\n",
       "       -4.57187705e-02, -1.77930854e-02, -7.16595277e-02, -7.34305149e-03,\n",
       "       -3.79458349e-03,  8.69043823e-03,  5.36074750e-02, -3.69317308e-02,\n",
       "        6.78913593e-02,  6.75247377e-03, -3.78016010e-02, -1.41637670e-08,\n",
       "       -5.67352101e-02, -4.07932419e-03,  7.57186487e-02,  7.59558007e-03,\n",
       "        4.98125814e-02, -3.34315300e-02,  1.13062793e-02,  6.92381188e-02,\n",
       "       -6.97447499e-03,  8.30163807e-03,  7.31794015e-02,  2.20696330e-02,\n",
       "       -1.57179553e-02,  9.77202505e-02,  2.31646816e-04, -9.94248223e-03,\n",
       "        5.39894141e-02,  9.65042971e-03,  1.12734390e-02, -4.04635854e-02,\n",
       "       -5.40181212e-02,  5.52954637e-02, -1.15938067e-01, -2.51252148e-02,\n",
       "        2.08169445e-02,  2.54807360e-02,  1.86110772e-02,  7.09262639e-02,\n",
       "        4.50047441e-02, -2.25867517e-02,  8.12138841e-02,  4.16815951e-02,\n",
       "       -9.02500972e-02, -4.54663336e-02, -3.52529483e-03,  6.87641203e-02,\n",
       "       -8.08570255e-03, -3.70551758e-02,  3.19098383e-02,  1.96891259e-02,\n",
       "        1.84312146e-02,  8.46597478e-02,  3.20015214e-02,  2.43468583e-02,\n",
       "        1.32686216e-02,  3.15156169e-02,  6.56160414e-02, -3.25499959e-02,\n",
       "        1.61910225e-02,  1.05654669e-03, -3.19140404e-03,  8.34278315e-02,\n",
       "        4.93808985e-02, -4.79649054e-03,  8.06483179e-02,  8.27923417e-03,\n",
       "        9.57589503e-03,  4.67768833e-02, -6.01301491e-02,  2.57616211e-02,\n",
       "        5.56659698e-02, -1.38345361e-02,  2.53715534e-02,  5.52726388e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query\n",
    "query = \"stopwords\"\n",
    "query_embedding = model.encode(query)\n",
    "query_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c74b17",
   "metadata": {},
   "source": [
    "BASES DE DATOS VECTORIALES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9537090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# Crear un índice FAISS para búsqueda de similitud\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
    "index.add(embeddings)  # Añadir los embeddings al índice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2480a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 63, 275, 468,  69,  85], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizar la búsqueda de similitud\n",
    "k = 5  # Número de resultados a recuperar\n",
    "D, I = index.search(np.array([query_embedding]), k)  # D: distancias, I: índices\n",
    "#guardar en top_k_indices los índices de los k resultados más cercanos\n",
    "top_k_indices = I[0]\n",
    "top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed5114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado 1:\n",
      "Página: 64\n",
      "Texto: bonline edition cn2009 cambridge upn22ndetermining vocabulary termsn27n222ndropping common terms stop wordsnsometimes extremely common words would appear ofnlittle value helping select documents match...\n",
      "Distancia: 0.981431245803833\n",
      "\n",
      "Resultado 2:\n",
      "Página: 276\n",
      "Texto: bonline edition cn2009 cambridge upn121nlanguage modelsn239nmodel m1nmodel m2nthen02nthen015nan01nan012nfrogn001nfrogn00002ntoadn001ntoadn00001nsaidn003nsaidn003nlikesn002nlikesn004nthatn004nthatn004n...\n",
      "Distancia: 1.1468493938446045\n",
      "\n",
      "Resultado 3:\n",
      "Página: 469\n",
      "Texto: bonline edition cn2009 cambridge upn432n19nweb search basicsn194nthe search user experiencenit crucial understand users web search well isnagain signixefxacx81cant change traditional information retri...\n",
      "Distancia: 1.1579183340072632\n",
      "\n",
      "Resultado 4:\n",
      "Página: 70\n",
      "Texto: bonline edition cn2009 cambridge upn22ndetermining vocabulary termsn33nlinguistic processing stemming lemmatization often done adnditional plugin component indexing process number suchncomponents exis...\n",
      "Distancia: 1.1599345207214355\n",
      "\n",
      "Resultado 5:\n",
      "Página: 86\n",
      "Texto: bonline edition cn2009 cambridge upndraft xc2xa9 april 1 2009 cambridge university press feedback welcomen49n3ndictionaries tolerantnretrievalnin chapters 1 2 developed ideas underlying inverted index...\n",
      "Distancia: 1.168470859527588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la búsqueda\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"Resultado {i + 1}:\")\n",
    "    print(f\"Página: {df.iloc[idx]['page']}\")\n",
    "    print(f\"Texto: {df.iloc[idx]['filtered'][:200]}...\")  # Mostrar solo los primeros 200 caracteres\n",
    "    print(f\"Distancia: {D[0][i]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61448ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto de los resultados más similares:\n",
      "bonline edition cn2009 cambridge upn22ndetermining vocabulary termsn27n222ndropping common terms stop wordsnsometimes extremely common words would appear ofnlittle value helping select documents matching user need excludednfrom vocabulary entirely words called stop words generalnstop wordsnstrategy determining stop list sort terms collection frequencyncollectionnfrequencynthe total number times term appears document collectionnand take frequent terms often handxefxacx81ltered senmantic content relative domain documents indexed asna stop list members discarded indexing annstop listnexample stop list shown figure 25 using stop list signixefxacx81cantlynreduces number postings system store presentnsome statistics chapter 5 see table 51 page 87 lot ofnthe time indexing stop words little harm keyword searches withnterms like donxe2x80x99t seem useful however true fornphrase searches phrase query xe2x80x9cpresident united statesxe2x80x9d conntains two stop words precise president xe2x80x9cunited statesxe2x80x9d thenmeaning xefxacx82ights london likely lost word stopped ansearch vannevar bushxe2x80x99s article may think difxefxacx81cult xefxacx81rstnthree words stopped system searches simply documentsncontaining word think special query types disproportionatelynaffected song titles well known pieces verse consist entirely ofnwords commonly stop lists let donxe2x80x99t wantnto nthe general trend ir systems time standard use ofnquite large stop lists 200xe2x80x93300 terms small stop lists 7xe2x80x9312 termsnto stop list whatsoever web search engines generally use stopnlists design modern ir systems focused precisely onnhow exploit statistics language able cope withncommon words better ways show section 53 page 95 howngood compression techniques greatly reduce cost storing postingsnfor common words section 621 page 117 discusses standardnterm weighting leads common words little impact docnument rankings finally section 715 page 140 shows ir systemnwith impactsorted indexes terminate scanning postings list early whennweights get small hence common words cause large additionalnprocessing cost average query even though postings lists stopnwords long modern ir systems additional cost ofnincluding stop words big xe2x80x93 neither terms index size innterms query processing timen bonline edition cn2009 cambridge upn121nlanguage modelsn239nmodel m1nmodel m2nthen02nthen015nan01nan012nfrogn001nfrogn00002ntoadn001ntoadn00001nsaidn003nsaidn003nlikesn002nlikesn004nthatn004nthatn004ndogn0005ndogn001ncatn0003ncatn0015nmonkeyn0001nmonkeyn0002nnnnnxe2x97xaefigure 123npartial specixefxacx81cation two unigram language modelsnx0fnexample 121nto xefxacx81nd probability word sequence multiply thenprobabilities model gives word sequence together thenprobability continuing stopping producing word examplenpfrog said toad likes frognn001 xc3x97 003 xc3x97 004 xc3x97 001 xc3x97 002 xc3x97 001n122nxc3x9708 xc3x97 08 xc3x97 08 xc3x97 08 xc3x97 08 xc3x97 08 xc3x97 02nxe2x89x88n0000000000001573nas see probability particular stringdocument usually verynsmall number stopped generating frog second time xefxacx81rst line ofnnumbers term emission probabilities second line gives probabilnity continuing stopping generating word explicit stop probabilitynis needed xefxacx81nite automaton wellformed language model according tonequation 121 nevertheless time omit include stop andn1 xe2x88x92stop probabilities authors compare two models andata set calculate likelihood ratio results simply dividing thenlikelihood rationprobability data according one model probability data accordning model providing stop probability xefxacx81xed inclusion willnnot alter likelihood ratio results comparing likelihood two lannguage models generating string hence alter ranking documents2nnevertheless formally numbers longer truly probabilities onlynproportional probabilities see exercise 124nx0fnexample 122nsuppose two language models m1 m2nshown partially figure 123 gives probability estimate sequence ofn2 ir context leading taking stop probability xefxacx81xed acrossnmodels seems reasonable generating queries length distributionnof queries xefxacx81xed independent document generating languagenmodeln bonline edition cn2009 cambridge upn432n19nweb search basicsn194nthe search user experiencenit crucial understand users web search well isnagain signixefxacx81cant change traditional information retrieval usersnwere typically professionals least training art phrasingnqueries wellauthored collection whose style structure unnderstood well contrast web search users tend know care aboutnthe heterogeneity web content syntax query languages artnof phrasing queries indeed mainstream tool web search come tonbecome place onerous demands billions people anrange studies concluded average number keywords anweb search somewhere 2 3 syntax operators boolean connnectives wildcards etc seldom used result compositionnof audience xe2x80x93 xe2x80x9cnormalxe2x80x9d people information scientistsnit clear user trafxefxacx81c web search engine attract thenmore revenue stands earn sponsored search search enngines differentiate grow trafxefxacx81c google identixefxacx81edntwo principles helped grow expense competitors 1 anfocus relevance specixefxacx81cally precision rather recall xefxacx81rst rensults 2 user experience lightweight meaning searchnquery page search results page uncluttered almost entirelyntextual graphical elements effect xefxacx81rst simplynto save users time locating information sought effect thensecond provide user experience extremely responsive anynrate bottlenecked time load search query results pagen1941nuser query needsnthere appear three broad categories common web searchnqueries grouped informational ii navigational iii transacntional explain categories clear queriesnwill fall one categories others fall outsidenthemninformational queries seek general information broad topic asninformationalnqueriesnleukemia provence typically single web page conntains information sought indeed users informational queriesntypically try assimilate information multiple web pagesnnavigational queries seek website home page single entity thennavigationalnqueriesnuser mind say lufthansa airlines cases userxe2x80x99s expectationnis xefxacx81rst search result home page lufthansanthe user interested plethora documents containing termnlufthansa user best measure user satisfaction precision atn1n bonline edition cn2009 cambridge upn22ndetermining vocabulary termsn33nlinguistic processing stemming lemmatization often done adnditional plugin component indexing process number suchncomponents exist commercial opensourcenthe common algorithm stemming english one renpeatedly shown empirically effective porterxe2x80x99s algorithmnporter stemmernporter 1980 entire algorithm long intricate present herenbut indicate general nature porterxe2x80x99s algorithm consists 5 phasesnof word reductions applied sequentially within phase varnious conventions select rules selecting rule rulengroup applies longest sufxefxacx81x xefxacx81rst phase convention isnused following rule groupn21nrulenexamplenssesnxe2x86x92nssncaressesnxe2x86x92ncaressniesnxe2x86x92ninponiesnxe2x86x92nponinssnxe2x86x92nssncaressnxe2x86x92ncaressnsnxe2x86x92ncatsnxe2x86x92ncatnmany later rules use concept measure word looselynchecks number syllables see whether word long enough itnis reasonable regard matching portion rule sufxefxacx81x rather thannas part stem word example rulenm 1nementnxe2x86x92nwould map replacement replac cement c ofxefxacx81cial site thenporter stemmer isnhttpwwwtartarusorgxcbx9cmartinporterstemmernother stemmers exist including older onepass lovins stemmer lovinsn1968 newer entrants like paicehusk stemmer paice 1990 seenhttpwwwcswaikatoacnzxcbx9ceibestemmersnhttpwwwcomplancsacukcomputingresearchstemmingnfigure 28 presents informal comparison different behaviors thesenstemmers stemmers use languagespecixefxacx81c rules require less knownledge lemmatizer needs complete vocabulary morphonlogical analysis correctly lemmatize words particular domains may alsonrequire special stemming rules however exact stemmed form notnmatter equivalence classes formsnrather using stemmer use lemmatizer tool natnlemmatizernural language processing full morphological analysis accunrately identify lemma word full morphological analysisnproduces modest benexefxacx81ts retrieval hard say moren bonline edition cn2009 cambridge upndraft xc2xa9 april 1 2009 cambridge university press feedback welcomen49n3ndictionaries tolerantnretrievalnin chapters 1 2 developed ideas underlying inverted indexesnfor handling boolean proximity queries develop techniquesnthat robust typographical errors query well alternativenspellingsnin section 31 develop data structures help searchnfor terms vocabulary inverted index section 32 studynthe idea wildcard query query aeiou seeks docnwildcard querynuments containing term includes xefxacx81ve vowels sequencenthe symbol indicates possibly empty string characters users posensuch queries search engine uncertain spellna query term seek documents containing variants query term innstance query automat would seek documents containing termsnautomatic automation automatednwe turn forms imprecisely posed queries focusing onnspelling errors section 33 users make spelling errors either accidentnor term searching eg herman unambiguousnspelling collection detail number techniques correctingnspelling errors queries one term time well entire stringnof query terms finally section 34 study method seeking voncabulary terms phonetically close query terms benespecially useful cases like herman example user may notnknow proper name spelled documents collectionnbecause develop many variants inverted indexes chapternwe use sometimes phrase standard inverted index mean invertednindex developed chapters 1 2 vocabulary term anpostings list documents collectionn31nsearch structures dictionariesngiven inverted index query xefxacx81rst task determine whetherneach query term exists vocabulary identify pointer\n"
     ]
    }
   ],
   "source": [
    "#unir los 5 resultados más similares\n",
    "context = \" \".join(df.iloc[index]['filtered'] for index in top_k_indices)\n",
    "# Imprimir el contexto\n",
    "print(\"Contexto de los resultados más similares:\")\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e33b3",
   "metadata": {},
   "source": [
    "INTEGRACIÓN CON IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4192ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Eres una aplicación de Retrieval Augmented Generation que siempre responde en español. Usa el siguiente contexto para responder la pregunta.\n",
    "Si la respuesta no está en el contexto, di que no sabes.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "El usuario está preguntando sobre: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd079e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Las \"stop words\" o palabras vacías son términos muy comunes en un idioma que a menudo se excluyen del procesamiento de texto porque tienen poco valor para la selección de documentos que coincidan con las necesidades del usuario. Estas palabras se descartan para optimizar el proceso de indexación y búsqueda, ya que su inclusión puede aumentar significativamente los costos de almacenamiento y procesamiento en los sistemas de recuperación de información (IR). Ejemplos de stop words son palabras como \"y\", \"de\", \"la\", que no aportan información significativa en búsquedas y se suelen eliminar de los índices para mejorar la eficiencia. Aunque esto puede no afectar mucho las búsquedas por palabras clave, en consultas de frases, la eliminación de stop words puede llevar a perder el significado completo de la búsqueda.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#guardar el resultado en una variable\n",
    "respuesta = completion.choices[0].message.content\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0465da3",
   "metadata": {},
   "source": [
    "TEXT TO SPEECH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa333d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎧 Reproduciendo audio...\n",
      "✅ Reproducción finalizada.\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import io\n",
    "import pygame\n",
    "\n",
    "# Convertir texto a MP3 (en memoria)\n",
    "tts = gTTS(text=respuesta, lang='es')\n",
    "mp3_fp = io.BytesIO()\n",
    "tts.write_to_fp(mp3_fp)\n",
    "mp3_fp.seek(0)\n",
    "\n",
    "# Inicializar pygame para reproducir\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(mp3_fp)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "print(\"🎧 Reproduciendo audio...\")\n",
    "# Esperar a que termine la reproducción\n",
    "while pygame.mixer.music.get_busy():\n",
    "    pygame.time.Clock().tick(10)\n",
    "#imprimir mensaje de finalización\n",
    "print(\"✅ Reproducción finalizada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e8e9a",
   "metadata": {},
   "source": [
    "TEXT TO SPEECH GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9197da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import pygame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63c2ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar pygame mixer\n",
    "pygame.mixer.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"\"\n",
    "url = f\"https://texttospeech.googleapis.com/v1/text:synthesize?key={API_KEY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024212f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎧 Reproduciendo audio...\n",
      "✅ Reproducción finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Cuerpo de la solicitud\n",
    "data = {\n",
    "    \"input\": {\"text\": respuesta},\n",
    "    \"voice\": {\"languageCode\": \"es-ES\", \"ssmlGender\": \"NEUTRAL\"},\n",
    "    \"audioConfig\": {\"audioEncoding\": \"MP3\"}\n",
    "}\n",
    "\n",
    "# Enviar la solicitud a la API\n",
    "response = requests.post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Decodificar el contenido base64\n",
    "    audio_base64 = response.json()[\"audioContent\"]\n",
    "    audio_data = base64.b64decode(audio_base64)\n",
    "\n",
    "    # Cargar el audio en memoria usando un objeto BytesIO\n",
    "    audio_stream = io.BytesIO(audio_data)\n",
    "\n",
    "    # Cargar en pygame\n",
    "    pygame.mixer.music.load(audio_stream)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    print(\"🎧 Reproduciendo audio...\")\n",
    "\n",
    "    # Esperar a que termine la reproducción\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        continue\n",
    "    #imprimir mensaje de finalización\n",
    "    print(\"✅ Reproducción finalizada.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error al obtener audio:\", response.text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
